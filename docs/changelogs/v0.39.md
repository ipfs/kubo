# Kubo changelog v0.39

<a href="https://ipshipyard.com/"><img align="right" src="https://github.com/user-attachments/assets/39ed3504-bb71-47f6-9bf8-cb9a1698f272" /></a>

This release was brought to you by the [Shipyard](https://ipshipyard.com/) team.

- [v0.39.0](#v0390)

## v0.39.0

- [Overview](#overview)
- [ğŸ”¦ Highlights](#-highlights)
  - [ğŸ“Š Detailed statistics for Sweep provider with `ipfs provide stat`](#-detailed-statistics-for-sweep-provider-with-ipfs-provide-stat)
  - [â¯ï¸ Provider resume cycle for improved reproviding reliability](#provider-resume-cycle-for-improved-reproviding-reliability)
  - [ğŸ”” Sweep provider slow reprovide warnings](#-sweep-provider-slow-reprovide-warnings)
  - [ğŸ”§ Fixed UPnP port forwarding after router restarts](#-fixed-upnp-port-forwarding-after-router-restarts)
  - [ğŸ–¥ï¸ RISC-V support with prebuilt binaries](#ï¸-risc-v-support-with-prebuilt-binaries)
  - [ğŸª¦ Deprecated `go-ipfs` name no longer published](#-deprecated-go-ipfs-name-no-longer-published)
- [ğŸ“¦ï¸ Important dependency updates](#-important-dependency-updates)
- [ğŸ“ Changelog](#-changelog)
- [ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Contributors](#-contributors)

### Overview

### ğŸ”¦ Highlights

#### ğŸ“Š Detailed statistics for Sweep provider with `ipfs provide stat`

The experimental Sweep provider system ([introduced in
v0.38](https://github.com/ipfs/kubo/blob/master/docs/changelogs/v0.38.md#-experimental-sweeping-dht-provider))
now has detailed statistics available through `ipfs provide stat`.

These statistics help you monitor provider health and troubleshoot issues,
especially useful for nodes providing large content collections. You can quickly
identify bottlenecks like queue backlog, worker saturation, or connectivity
problems that might prevent content from being announced to the DHT.

**Default behavior:** Displays a brief summary showing queue sizes, scheduled
CIDs/regions, average record holders, ongoing/total provides, and worker status
when resources are constrained.

**Detailed statistics with `--all`:** View complete metrics organized into sections:

- **Connectivity**: DHT connection status
- **Queues**: Pending provide and reprovide operations
- **Schedule**: CIDs/regions scheduled for reprovide
- **Timings**: Uptime, reprovide cycle information
- **Network**: Peer statistics, keyspace region sizes
- **Operations**: Ongoing and past provides, rates, errors
- **Workers**: Worker pool utilization and availability

**Real-time monitoring:** For continuous monitoring, run
`watch ipfs provide stat --all --compact` to see detailed statistics refreshed
in a 2-column layout. This lets you observe provide rates, queue sizes, and
worker availability in real-time. Individual sections can be displayed using
flags like `--network`, `--operations`, or `--workers`, and multiple flags can
be combined for custom views.

**Dual DHT support:** For Dual DHT configurations, use `--lan` to view LAN DHT
provider statistics instead of the default WAN DHT stats.

> [!NOTE]
> These statistics are only available when using the Sweep provider system
> (enabled via
> [`Provide.DHT.SweepEnabled`](https://github.com/ipfs/kubo/blob/master/docs/config.md#providedhtsweepenabled)).
> Legacy provider shows basic statistics without flag support.

#### â¯ï¸ Provider resume cycle for improved reproviding reliability

When using the sweeping provider (`Provide.DHT.SweepEnabled`), Kubo now
persists the reprovide cycle state and automatically resumes where it left off
after a restart. This brings several improvements:

- **Persistent progress**: The provider now saves its position in the reprovide
cycle to the datastore. On restart, it continues from where it stopped instead
of starting from scratch.
- **Catch-up reproviding**: If the node was offline for an extended period, all
CIDs that haven't been reprovided within the configured reprovide interval are
immediately queued for reproviding when the node starts up. This ensures
content availability is maintained even after downtime.
- **Persistent provide queue**: The provide queue is now persisted to the
datastore on shutdown. When the node restarts, queued CIDs are restored and
provided as expected, preventing loss of pending provide operations.
- **Resume control**: The resume behavior is now controlled via the
`Provide.DHT.ResumeEnabled` config option (default: `true`). If you don't want
to keep the persisted provider state from a previous run, you can set
`Provide.DHT.ResumeEnabled=false` in your config.

This feature significantly improves the reliability of content providing,
especially for nodes that experience intermittent connectivity or restarts.

#### ğŸ”” Sweep provider slow reprovide warnings

Kubo now monitors DHT reprovide operations when `Provide.DHT.SweepEnabled=true`
and alerts you if your node is falling behind on reprovides.

When the reprovide queue consistently grows and all periodic workers are busy,
a warning displays with:

- Queue size and worker utilization details
- Recommended solutions: increase `Provide.DHT.MaxWorkers` or `Provide.DHT.DedicatedPeriodicWorkers`
- Command to monitor real-time progress: `watch ipfs provide stat --all --compact`

The alert polls every 15 minutes (to avoid alert fatigue while catching
persistent issues) and only triggers after sustained growth across multiple
intervals. The legacy provider is unaffected by this change.

#### ğŸ”§ Fixed UPnP port forwarding after router restarts

Kubo now automatically recovers UPnP port mappings when routers restart or
become temporarily unavailable, fixing a critical connectivity issue that
affected self-hosted nodes behind NAT.

**Previous behavior:** When a UPnP-enabled router restarted, Kubo would lose
its port mapping and fail to re-establish it automatically. Nodes would become
unreachable to the network until the daemon was manually restarted, forcing
reliance on relay connections which degraded performance.

**New behavior:** The upgraded go-libp2p (v0.44.0) includes [Shipyard's fix](https://github.com/libp2p/go-libp2p/pull/3367)
for self-healing NAT mappings that automatically rediscover and re-establish
port forwarding after router events. Nodes now maintain public connectivity
without manual intervention.

> [!NOTE]
> If your node runs behind a router and you haven't manually configured port
> forwarding, make sure [`Swarm.DisableNatPortMap=false`](https://github.com/ipfs/kubo/blob/master/docs/config.md#swarmdisablenatportmap)
> so UPnP can automatically handle port mapping (this is the default).

This significantly improves reliability for desktop and self-hosted IPFS nodes
using UPnP for NAT traversal.

#### ğŸ–¥ï¸ RISC-V support with prebuilt binaries

Kubo now provides official `linux-riscv64` prebuilt binaries with every release,
bringing IPFS to [RISC-V](https://en.wikipedia.org/wiki/RISC-V) open hardware.

As RISC-V single-board computers and embedded systems become more accessible,
it's good to see the distributed web supported on open hardware architectures -
a natural pairing of open technologies.

Download from <https://dist.ipfs.tech/kubo/> or
<https://github.com/ipfs/kubo/releases> and look for the `linux-riscv64` archive.

#### ğŸª¦ Deprecated `go-ipfs` name no longer published

The `go-ipfs` name was deprecated in 2022 and renamed to `kubo`. Starting with this release, we have stopped publishing Docker images and distribution binaries under the old `go-ipfs` name.

Existing users should switch to:

- Docker: `ipfs/kubo` image (instead of `ipfs/go-ipfs`)
- Binaries: download from <https://dist.ipfs.tech/kubo/> or <https://github.com/ipfs/kubo/releases>

For Docker users, the legacy `ipfs/go-ipfs` image name now shows a deprecation notice directing you to `ipfs/kubo`.

### ğŸ“¦ï¸ Important dependency updates

- update `go-libp2p` to [v0.44.0](https://github.com/libp2p/go-libp2p/releases/tag/v0.44.0) with self-healing UPnP port mappings
- update `quic-go` to [v0.55.0](https://github.com/quic-go/quic-go/releases/tag/v0.55.0)
- update `go-ds-pebble` to [v0.5.6](https://github.com/ipfs/go-ds-pebble/releases/tag/v0.5.6) (includes pebble [v2.1.1](https://github.com/cockroachdb/pebble/releases/tag/v2.1.1))

### ğŸ“ Changelog

### ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Contributors
